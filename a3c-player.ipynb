{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from retro_contest.local import make\n",
    "import torch, os, gym, time, glob, argparse, sys\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter\n",
    "from scipy.misc import imresize # preserves single-pixel info _unlike_ img = img[::2,::2]\n",
    "from gym import spaces\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import tensorboardX\n",
    "from matplotlib import pyplot as pyplot\n",
    "from tensorboardX import SummaryWriter\n",
    "from baselines.common.atari_wrappers import FrameStack\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--env', default='Pong-v4', type=str, help='gym environment')\n",
    "    parser.add_argument('--processes', default=mp.cpu_count(), type=int, help='number of processes to train with')\n",
    "    parser.add_argument('--render', default=False, type=bool, help='renders the atari environment')\n",
    "    parser.add_argument('--test', default=False, type=bool, help='sets lr=0, chooses most likely actions')\n",
    "    parser.add_argument('--rnn_steps', default=20, type=int, help='steps to train LSTM over')\n",
    "    parser.add_argument('--lr', default=2e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--seed', default=1, type=int, help='seed random # generators (for reproducibility)')\n",
    "    parser.add_argument('--gamma', default=0.99, type=float, help='rewards discount factor')\n",
    "    parser.add_argument('--tau', default=1.0, type=float, help='generalized advantage estimation discount')\n",
    "    parser.add_argument('--horizon', default=0.99, type=float, help='horizon for running averages')\n",
    "    parser.add_argument('--hidden', default=256, type=int, help='hidden size of GRU')\n",
    "    parser.add_argument('--sonic', default=False, type=bool, help='sonic')\n",
    "    return parser.parse_args()\n",
    "    \n",
    "args = get_args()\n",
    "args.save_dir = '{}/'.format(args.env.lower()) # keep the directory structure simple\n",
    "writer = SummaryWriter(\"./\" + args.save_dir + \"/runs\")\n",
    "\n",
    "discount = lambda x, gamma: lfilter([1],[1,-gamma],x[::-1])[::-1] # discounted rewards one liner\n",
    "\n",
    "#can be multiples of 16 starting at 80 (80, 96, 112, 128, etc.)\n",
    "state_width = 80\n",
    "state_height = 80\n",
    "\n",
    "#prepro = lambda img: imresize(img[35:195].mean(2), (80,80)).astype(np.float32).reshape(1,80,80)/255.\n",
    "\n",
    "def prepro(img):\n",
    "    return img\n",
    "    \n",
    "def printlog(args, s, end='\\n', mode='a'):\n",
    "    print(s, end=end) ; #f=open(args.save_dir+'log.txt',mode) ; f.write(s+'\\n') ; f.close()\n",
    "    \n",
    "def make_env(game, state=None, stack=False, scale_rew=False):\n",
    "    \"\"\"\n",
    "    Create an environment with some standard wrappers.\n",
    "    \"\"\"\n",
    "    if(state==None):\n",
    "        env = gym.make(game)\n",
    "    else: env = make(game=game, state=state); env = SonicDiscretizer(env); #env = AllowBacktracking(env)\n",
    "    if scale_rew:\n",
    "        env = RewardScaler(env)\n",
    "    env = WarpFrame(env)\n",
    "    if stack:\n",
    "        env = FrameStack(env, 4)\n",
    "    return env\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = state_width\n",
    "        self.height = state_height\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "            shape=(1, self.height, self.width), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)/255.0\n",
    "        return frame[:, :, None]\n",
    "\n",
    "class SonicDiscretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(SonicDiscretizer, self).__init__(env)\n",
    "        buttons = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * 12)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()\n",
    "\n",
    "class RewardScaler(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Bring rewards to a reasonable scale for PPO.\n",
    "    This is incredibly important and effects performance\n",
    "    drastically.\n",
    "    \"\"\"\n",
    "    def reward(self, reward):\n",
    "        return reward * 0.01\n",
    "\n",
    "class AllowBacktracking(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Use deltas in max(X) as the reward, rather than deltas\n",
    "    in X. This way, agents are not discouraged too heavily\n",
    "    from exploring backwards if there is no way to advance\n",
    "    head-on in the level.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(AllowBacktracking, self).__init__(env)\n",
    "        self._cur_x = 0\n",
    "        self._max_x = 0\n",
    "\n",
    "    def reset(self, **kwargs): # pylint: disable=E0202\n",
    "        self._cur_x = 0\n",
    "        self._max_x = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action): # pylint: disable=E0202\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        self._cur_x += rew\n",
    "        rew = max(0, self._cur_x - self._max_x)\n",
    "        self._max_x = max(self._max_x, self._cur_x)\n",
    "        return obs, rew, done, info\n",
    "        \n",
    "class NNPolicy(nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, channels, memsize, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.gru = nn.GRUCell(32 * 5 * 5* state_width/80 * state_height/80, memsize)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(memsize, 1), nn.Linear(memsize, num_actions)\n",
    "\n",
    "    def forward(self, inputs, train=True, hard=False):\n",
    "        inputs, hx = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        #print(x.norm())\n",
    "        #print(hx.norm())\n",
    "        #print(x.shape)\n",
    "        hx = self.gru(x.view(-1, 32 * 5 * 5 * state_width/80 * state_height/80), (hx))\n",
    "        #print(hx.norm())\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), hx\n",
    "\n",
    "    def try_load(self, save_dir):\n",
    "        paths = glob.glob(save_dir + '*.tar') ; step = 0\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [int(s.split('.')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; step = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if step is 0 else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return step\n",
    "\n",
    "class SharedAdam(torch.optim.Adam): # extend a pytorch optimizer so it shares grads across processes\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['shared_steps'], state['step'] = torch.zeros(1).share_memory_(), 0\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_().share_memory_()\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_().share_memory_()\n",
    "                \n",
    "        def step(self, closure=None):\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None: continue\n",
    "                    self.state[p]['shared_steps'] += 1\n",
    "                    self.state[p]['step'] = self.state[p]['shared_steps'][0] - 1 # a \"step += 1\"  comes later\n",
    "            super.step(closure)\n",
    "\n",
    "def cost_func(args, values, logps, actions, rewards, rank):\n",
    "    np_values = values.view(-1).data.numpy()\n",
    "    #np_values[-1] = bootstrap value\n",
    "    #np_values is values_plus\n",
    "    #rewards is rewards_plus\n",
    "    # generalized advantage estimation using \\delta_t residuals (a policy gradient method)\n",
    "    delta_t = np.asarray(rewards) + args.gamma * np_values[1:] - np_values[:-1] #advantage\n",
    "    logpys = logps.gather(1, torch.tensor(actions).view(-1,1))\n",
    "\n",
    "    gen_adv_est = discount(delta_t, args.gamma * args.tau) #final discounted advantage\n",
    "    policy_loss = -(logpys.view(-1) * torch.FloatTensor(gen_adv_est.copy())).sum() #negative sum of advantage * log-probabilities\n",
    "    \n",
    "    # l2 loss over value estimator\n",
    "    rewards[-1] += args.gamma * np_values[-1] #calculate discounted value \n",
    "    discounted_r = discount(np.asarray(rewards), args.gamma)\n",
    "    discounted_r = torch.tensor(discounted_r.copy(), dtype=torch.float32)\n",
    "    value_loss = .5 * (discounted_r - values[:-1,0]).pow(2).sum() #mean-squared error\n",
    "    #print(rewards)\n",
    "    #print(discounted_r)\n",
    "    entropy_loss = -(-logps * torch.exp(logps)).sum() # sum of probabilities * logprobabilities : encourage lower entropy\n",
    "    return policy_loss + 0.5 * value_loss + 0.01 * entropy_loss\n",
    "\n",
    "def train(shared_model, shared_optimizer, rank, args, info):\n",
    "    if args.sonic: env = make_env(game='SonicTheHedgehog-Genesis', state=args.env, stack=False, scale_rew=False)\n",
    "    else: env = make_env(args.env) # make a local (unshared) environment\n",
    "    env.seed(args.seed + rank) ; torch.manual_seed(args.seed + rank) # seed everything\n",
    "    model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions) # a local/unshared model\n",
    "    state = torch.tensor(prepro(env.reset())).type(torch.FloatTensor) # get first state\n",
    "    testImg = state.numpy()[:, :, 0]\n",
    "    start_time = last_disp_time = last_disp_time_2 = time.time()\n",
    "    episode_length, epr, eploss, done  = 0, 0, 0, True # bookkeeping\n",
    "    measure_counter = 0\n",
    "    bestReward = 0\n",
    "    lostProgress = 0\n",
    "    print(str(rank) + \"initialized\")\n",
    "    while info['frames'][0] <= 8e7 or args.test: # openai baselines uses 40M frames...we'll use 80M\n",
    "        model.load_state_dict(shared_model.state_dict()) # sync with shared model\n",
    "\n",
    "        hx = torch.zeros(1, 256) if done else hx.detach()  # rnn activation vector\n",
    "        values, logps, actions, rewards = [], [], [], [] # save values for computing gradientss\n",
    "        \n",
    "        for step in range(args.rnn_steps):\n",
    "            #print(state.shape)\n",
    "            #img = state.numpy()\n",
    "            #img = img.squeeze(2)\n",
    "            #pyplot.imshow(img, cmap=\"gray\")\n",
    "            #pyplot.show()\n",
    "            episode_length += 1\n",
    "\n",
    "            value, logit, hx = model((state.view(1,1,state_width,state_height).type(torch.FloatTensor), hx))\n",
    "            logp = F.log_softmax(logit, dim=-1)\n",
    "            action = torch.exp(logp).multinomial(num_samples=1).data[0]#logp.max(1)[1].data if args.test else\n",
    "            #if args.render:\n",
    "                #action = logp.max(1)[1].data\n",
    "            if(args.sonic): \n",
    "                a = action.numpy()[0]\n",
    "                b = np.zeros(args.num_actions, dtype=int)\n",
    "                b[a] = 1\n",
    "                if rank == 0:\n",
    "                    #print(state)\n",
    "                    #print(logp)\n",
    "                    pass\n",
    "                state, reward, done, _ = env.step(action.numpy()[0])\n",
    "                #if rank == 0:\n",
    "                #    print(reward)\n",
    "            else:\n",
    "                state, reward, done, _ = env.step(action.numpy()[0])\n",
    "            if args.render: env.render()\n",
    "\n",
    "            state = torch.tensor(prepro(state)).type(torch.FloatTensor) ; epr += reward\n",
    "            \n",
    "            #if(epr > bestReward):\n",
    "            #    bestReward = epr\n",
    "            #    lostProgress = 0\n",
    "            #else:\n",
    "            #    lostProgress += 1\n",
    "\n",
    "            #if lostProgress > 200:\n",
    "            #    lostProgress = 0\n",
    "            #    done = True\n",
    "                \n",
    "            #print(reward)\n",
    "            if args.sonic:\n",
    "                reward = np.clip(reward, -1, 1) # TEST THIS, TRY CHANGING -1 to 0 OR 2 BACK TO 1 AND THE NEW CODE COMMENTED BELOW\n",
    "            else:\n",
    "                reward = np.clip(reward, -1, 1)\n",
    "            done = done or episode_length >= 1e4 # don't playing one ep for too long\n",
    "            \n",
    "            info['frames'].add_(1) ; num_frames = int(info['frames'].item())\n",
    "            if num_frames % 1e5 == 0: # save every 2M frames\n",
    "                printlog(args, '\\n\\t{:.0f}M frames: saved model\\n'.format(num_frames/1e6))\n",
    "                torch.save(shared_model.state_dict(), args.save_dir+'model.{:.0f}.tar'.format(num_frames/1e5))\n",
    "            \n",
    "            if done: # update shared data\n",
    "                if rank == 0:\n",
    "                    writer.add_scalar(\"rewards/reward_\" + str(rank), epr, info['frames'])\n",
    "                    writer.add_scalar(\"losses/loss_\" + str(rank), eploss, info['frames'])\n",
    "                    writer.add_scalar(\"rewards/running_reward\", info['run_epr'].item(), info['frames'])\n",
    "                    writer.add_scalar(\"losses/running_loss\", info['run_loss'].item(), info['frames'])\n",
    "                    writer.add_scalar(\"rewards/value\", str(value[0][0].item()), info['frames'])\n",
    "                    time.sleep(1)\n",
    "                    printlog(args, \"Data written for \" + str(rank))\n",
    "                    printlog(args, \"Reward: \" + str(epr))\n",
    "                info['episodes'] += 1\n",
    "                interp = 1 if info['episodes'][0] == 1 else 1 - args.horizon\n",
    "                info['run_epr'].mul_(1-interp).add_(interp * epr)\n",
    "                info['run_loss'].mul_(1-interp).add_(interp * eploss)\n",
    "                \n",
    "            if rank == 0 and time.time() - last_disp_time_2 > 1:\n",
    "                printlog(args, '{:.0f} f/s'.format(num_frames - measure_counter))\n",
    "                measure_counter = int(info['frames'].item())\n",
    "                last_disp_time_2 = time.time()\n",
    "                \n",
    "            if rank == 0 and time.time() - last_disp_time > 60: # print info ~ every minute\n",
    "                elapsed = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - start_time))\n",
    "                printlog(args, 'time {}, episodes {:.0f}, frames {:.1f}M, mean epr {:.2f}, run loss {:.2f}'\n",
    "                    .format(elapsed, info['episodes'].item(), num_frames/1e6,\n",
    "                    info['run_epr'].item(), info['run_loss'].item()))\n",
    "                last_disp_time = time.time()\n",
    "\n",
    "            if done: # maybe print info.\n",
    "                episode_length, epr, eploss, bestReward = 0, 0, 0, 0\n",
    "                state = torch.tensor(prepro(env.reset()))\n",
    "\n",
    "            values.append(value) ; logps.append(logp) ; actions.append(action) ; rewards.append(reward)\n",
    "            if done: #NEW TEST THIS\n",
    "                break #NEW TEST THIS\n",
    "        #next_value = torch.zeros(1,1) if done else model((state.unsqueeze(0).view(1, 1, state_width, state_height), hx))[0]\n",
    "        #print(torch.zeros(1, 1))\n",
    "        #print(torch.from_numpy(np.array([[epr]])).type(torch.FloatTensor) )\n",
    "        next_value = torch.from_numpy(np.array([[epr]])).type(torch.FloatTensor) if done else model((state.unsqueeze(0).view(1, 1, state_width, state_height), hx))[0]\n",
    "        values.append(next_value.detach())\n",
    "        \n",
    "\n",
    "        loss = cost_func(args, torch.cat(values), torch.cat(logps), torch.cat(actions), np.asarray(rewards), rank)\n",
    "        eploss += loss.item()\n",
    "\n",
    "        if(args.lr != 0):\n",
    "            shared_optimizer.zero_grad() ; loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
    "    \n",
    "            for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "                if shared_param.grad is None: shared_param._grad = param.grad # sync gradients with shared model\n",
    "            shared_optimizer.step()\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    if sys.version_info[0] > 2:\n",
    "        mp.set_start_method('spawn', force=True) # this must not be in global scope\n",
    "    elif sys.platform == 'linux' or sys.platform == 'linux2':\n",
    "        raise \"Must be using Python 3 with linux!\" # or else you get a deadlock in conv2d\n",
    "    \n",
    "    args = get_args()\n",
    "    args.save_dir = '{}/'.format(args.env.lower()) # keep the directory structure simple\n",
    "    if args.render:  args.processes = 1 ; args.test = True # render mode -> test mode w one process\n",
    "    if args.test:  args.lr = 0 # don't train in render mode\n",
    "    \n",
    "    if args.sonic: args.num_actions = make_env(game='SonicTheHedgehog-Genesis', state=args.env, stack=False, scale_rew=False).action_space.n # get the action space of this game\n",
    "    else: args.num_actions = make_env(args.env).action_space.n \n",
    "    \n",
    "    print(args.num_actions)\n",
    "    os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None # make dir to save models etc.\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    shared_model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions).share_memory()\n",
    "    shared_optimizer = SharedAdam(shared_model.parameters(), lr=args.lr)\n",
    "\n",
    "    info = {k: torch.DoubleTensor([0]).share_memory_() for k in ['run_epr', 'run_loss', 'episodes', 'frames']}\n",
    "    info['frames'] += shared_model.try_load(args.save_dir) * 1e5\n",
    "    if int(info['frames'].item()) == 0: printlog(args,'', end='', mode='w') # clear log file\n",
    "    processes = []\n",
    "    for rank in range(args.processes):\n",
    "        p = mp.Process(target=train, args=(shared_model, shared_optimizer, rank, args, info))\n",
    "        p.start() ; processes.append(p)\n",
    "    for p in processes: p.join()\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
